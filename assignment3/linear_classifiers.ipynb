{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Assignment 4: Implementing linear classifiers</h1>\n",
    "<p>Andreas Koj</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "X1 = [{'city':'Gothenburg', 'month':'July'},\n",
    "      {'city':'Gothenburg', 'month':'December'},\n",
    "      {'city':'Paris', 'month':'July'},\n",
    "      {'city':'Paris', 'month':'December'}]\n",
    "Y1 = ['rain', 'rain', 'sun', 'rain']\n",
    "\n",
    "X2 = [{'city':'Sydney', 'month':'July'},\n",
    "      {'city':'Sydney', 'month':'December'},\n",
    "      {'city':'Paris', 'month':'July'},\n",
    "      {'city':'Paris', 'month':'December'}]\n",
    "Y2 = ['rain', 'sun', 'sun', 'rain']\n",
    "\n",
    "classifier1 = make_pipeline(DictVectorizer(), Perceptron(max_iter=10))\n",
    "classifier1.fit(X1, Y1)\n",
    "guesses1 = classifier1.predict(X1)\n",
    "print(accuracy_score(Y1, guesses1))\n",
    "\n",
    "#classifier2 = make_pipeline(DictVectorizer(), Perceptron(max_iter=10))\n",
    "classifier2 = make_pipeline(DictVectorizer(), SVC())\n",
    "\n",
    "classifier2.fit(X2, Y2)\n",
    "guesses2 = classifier2.predict(X2)\n",
    "print(accuracy_score(Y2, guesses2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise question answer:\n",
    "\n",
    "Perceptron and LinearSVC are linear classifiers, the output of this functions is a hyperplane which seperates two classes in this case (rain,sun). The output 50% means that the sperator (function) has problem to devide these classes (The problem is not linear seperable). To solve this problem we can use non linear classifier like for example SVC with default radial basis function kernel.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](data/p1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>LinearSVC implementation</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(BaseEstimator):\n",
    "    \"\"\"\n",
    "    General class for binary linear classifiers. Implements the predict\n",
    "    function, which is the same for all binary linear classifiers. There are\n",
    "    also two utility functions.\n",
    "    \"\"\"\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"\n",
    "        Computes the decision function for the inputs X. The inputs are assumed to be\n",
    "        stored in a matrix, where each row contains the features for one\n",
    "        instance.\n",
    "        \"\"\"\n",
    "        return X.dot(self.w)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the outputs for the inputs X. The inputs are assumed to be\n",
    "        stored in a matrix, where each row contains the features for one\n",
    "        instance.\n",
    "        \"\"\"\n",
    "\n",
    "        # First compute the output scores\n",
    "        scores = self.decision_function(X)\n",
    "\n",
    "        # Select the positive or negative class label, depending on whether\n",
    "        # the score was positive or negative.\n",
    "        out = np.select([scores >= 0.0, scores < 0.0],\n",
    "                        [self.positive_class,\n",
    "                         self.negative_class])\n",
    "        return out\n",
    "\n",
    "    def find_classes(self, Y):\n",
    "        \"\"\"\n",
    "        Finds the set of output classes in the output part Y of the training set.\n",
    "        If there are exactly two classes, one of them is associated to positive\n",
    "        classifier scores, the other one to negative scores. If the number of\n",
    "        classes is not 2, an error is raised.\n",
    "        \"\"\"\n",
    "        classes = sorted(set(Y))\n",
    "        if len(classes) != 2:\n",
    "            raise Exception(\"this does not seem to be a 2-class problem\")\n",
    "        self.positive_class = classes[1]\n",
    "        self.negative_class = classes[0]\n",
    "\n",
    "    def encode_outputs(self, Y):\n",
    "        \"\"\"\n",
    "        A helper function that converts all outputs to +1 or -1.\n",
    "        \"\"\"\n",
    "        return np.array([1 if y == self.positive_class else -1 for y in Y])\n",
    "\n",
    "\n",
    "class linearSVC(LinearClassifier):\n",
    "\n",
    "    def __init__(self, n_iter=20, lam=1):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.lam = lam\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        self.w = np.zeros(n_features)\n",
    "    \n",
    "        # The Pegasos algorithm with hinge loss. \n",
    "        t = 0\n",
    "        for i in range(self.n_iter):\n",
    "            for x, y in zip(X, Ye):\n",
    "                t = t+1\n",
    "                n = 1/(self.lam*t)\n",
    "                \n",
    "                score = x.dot(self.w)\n",
    "                \n",
    "                if y*score < 1:\n",
    "                    self.w = (1-n*self.lam)*self.w + (n*y)*x\n",
    "                else:\n",
    "                    self.w = (1-n*self.lam)*self.w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Training custom Linear SVC</h1>\n",
    "<p>I chose lamba= 0.0001 experimentally which gave me the best accuracy almost 84%</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 4.78 sec.\n",
      "Accuracy: 0.8393.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#from aml_perceptron import Perceptron, SparsePerceptron\n",
    "\n",
    "# This function reads the corpus, returns a list of documents, and a list\n",
    "# of their corresponding polarity labels. \n",
    "def read_data(corpus_file):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(corpus_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            _, y, _, x = line.split(maxsplit=3)\n",
    "            X.append(x.strip())\n",
    "            Y.append(y)\n",
    "    return X, Y\n",
    "\n",
    " \n",
    "# Read all the documents.\n",
    "X, Y = read_data('data/all_sentiment_shuffled.txt')\n",
    "\n",
    "# Split into training and test parts.\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,\n",
    "                                                random_state=0)\n",
    "\n",
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SelectKBest(k=2000),\n",
    "    Normalizer(),\n",
    "\n",
    "    linearSVC(lam=0.0001, n_iter=30)  \n",
    ")\n",
    "\n",
    "# Train the classifier.\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "\n",
    "# Evaluate on the test set.\n",
    "Yguess = pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Training custom Logistic Regression classifier</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logisticRegression(LinearClassifier):\n",
    "\n",
    "    def __init__(self, n_iter=20, lam=1):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.lam = lam\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        self.w = np.zeros(n_features)\n",
    "        \n",
    "        # The Pegasos algorithm with log loss \n",
    "        t = 0\n",
    "        for i in range(self.n_iter):\n",
    "            for x, y in zip(X, Ye):\n",
    "                t = t+1\n",
    "                n = 1/(self.lam*t)\n",
    "                \n",
    "                score = x.dot(self.w)\n",
    "\n",
    "                self.w = ((1-n*self.lam)*self.w) +((y / (1 + np.exp(y * (score))))* x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 5.80 sec.\n",
      "Accuracy: 0.8405.\n"
     ]
    }
   ],
   "source": [
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SelectKBest(k=2200),\n",
    "    Normalizer(),\n",
    "    logisticRegression(lam=0.1, n_iter=20)  \n",
    ")\n",
    "\n",
    "# Train the classifier.\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "\n",
    "# Evaluate on the test set.\n",
    "Yguess = pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Bonus task 1. Making your code more efficient</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linearSVC_fast(LinearClassifier):\n",
    "\n",
    "    def __init__(self, n_iter=20, lam=1):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.lam = lam\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        self.w = np.zeros(n_features)\n",
    "    \n",
    "        # The Pegasos algorithm with hinge loss. \n",
    "        t = 0\n",
    "        for i in range(self.n_iter):\n",
    "            for x, y in zip(X, Ye):\n",
    "                t = t+1\n",
    "                n = 1/(self.lam*t)\n",
    "                \n",
    "                score = x.dot(self.w)\n",
    "                \n",
    "                if y*score < 1:\n",
    "                    self.w = (1-n*self.lam)*self.w + (n*y)*x\n",
    "                else:\n",
    "                    self.w = (1-n*self.lam)*self.w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "avg time 4,5 - 5.5s <-- slow one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 4.25 sec.\n",
      "Accuracy: 0.7319.\n"
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SelectKBest(k=2000),\n",
    "    Normalizer(),\n",
    "    linearSVC_fast(lam=0.1, n_iter=20)  \n",
    ")\n",
    "\n",
    "# Train the classifier.\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "\n",
    "# Evaluate on the test set.\n",
    "Yguess = pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
