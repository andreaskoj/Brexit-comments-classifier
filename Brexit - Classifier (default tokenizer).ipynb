{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tips: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "\n",
    "\n",
    "1 Word removal-experiment with word removals both in high frequency and low frequency space\n",
    "\n",
    "2 Taxonomy- Ordered heirarchial taxonomy can help you reduce entropy in content. Similar to what decision trees accomplish. This is SUPER unappreciated in most text analytics literature\n",
    "\n",
    "3 Lemmatization. _ it sometimes helps improve structure of text\n",
    "\n",
    "4 Sampling - Are your classes balanced? if not resample, create synthetic data to populate low frequency classes\n",
    "\n",
    "5 ensemble/boosting/bagging methods- Have you tried meta algos to improve your performance?\n",
    "\n",
    "6 Data Quality check- Good data is like good food. Have you checked if you have any useless data? pruning data set helps improve structure a lot!\n",
    "\n",
    "7 Hyper Parameter tuning- Did you exhaustively search the subspaces to fine tune the model. Most models show upto 10% improvement by just hyperparameter tuning.\n",
    "\n",
    "8 Word Gram conversion- you can improve feature set by converting important bigrams and trigrams into unigrams \"food processing\" can be changed to \"food_processing\"\n",
    "\n",
    "9 Data preparation- Count vectors, tf-idf, word2vec .. et al.. Naive bayes seems to work better Count vectors,\n",
    "\n",
    "10 Sample length- This is a quirk i noticed in text mining. if you are classifying sentences ranging from few words to lets say a paragraph. With more additional words algos like naive bayes dont work well due lack of convergence arising from positive hits they get from all words for more that one class to put it naively (pun intended :P)\n",
    "\n",
    "11 Last point I can think is instead of getting class assignment get class probabilities and use that itself a feature in your ensembles. It seems to work in a variety of cases.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Read the data from the file </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(filename, encoding = 'utf-8') as f:\n",
    "        for l in f:\n",
    "            cols = l.split('\\t')\n",
    "            X.append(cols[1])\n",
    "            Y.append(cols[0])\n",
    "    return X, Y\n",
    "Xtrain, Ytrain = read_data('dataset/a2_first_sample.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Helpers</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns avg of results (no matter how many folds)         \n",
    "def avg(n , results):\n",
    "    top = 0\n",
    "    for r in results:\n",
    "        top += r\n",
    "\n",
    "    print(n, '\\t', top/len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Find the best classfier </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier \t 0.6387073229306476\n",
      "GradientBoostingClas \t 0.657723033270159\n",
      "LogisticRegressionCV \t 0.6677865071017489\n",
      "LogisticRegression \t 0.6934875151974657\n",
      "SVC               \t 0.5212529601694079\n",
      "SGDClassifier     \t 0.6364737259591845\n",
      "NuSVC             \t 0.6588002990103953\n",
      "DecisionTreeClassifier \t 0.6241508962061437\n",
      "ExtraTreeClassifier \t 0.5782589650404065\n",
      "CalibratedClasCV \t 0.6789421856049617\n",
      "Nearest Neighbors \t 0.6153185994154772\n",
      "BaggingClassifier \t 0.6241845150997957\n",
      "Linear SVM        \t 0.5212529601694079\n",
      "RBF SVM           \t 0.6969033031186518\n",
      "Decision Tree     \t 0.6163657353716686\n",
      "Random Forest     \t 0.5324350274257871\n",
      "Perceptron        \t 0.6487517641101923\n",
      "AdaBoost          \t 0.608426915157778\n",
      "Neural Network    \t 0.6644307268273808\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.tree import ExtraTreeClassifier, DecisionTreeClassifier\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "names = [\"ExtraTreesClassifier\",\n",
    "         \"GradientBoostingClas\",\n",
    "         \"LogisticRegressionCV\",\n",
    "         \"LogisticRegression\",\n",
    "         \"SVC              \",\n",
    "         \"SGDClassifier    \",\n",
    "         \"NuSVC            \",\n",
    "         \"DecisionTreeClassifier\",\n",
    "         \"ExtraTreeClassifier\",\n",
    "         \"CalibratedClasCV\",\n",
    "         \"Nearest Neighbors\",\n",
    "         \"BaggingClassifier\",\n",
    "         \"Linear SVM       \",\n",
    "         \"RBF SVM          \",\n",
    "         \"Decision Tree    \",\n",
    "         \"Random Forest    \",\n",
    "         \"Perceptron       \",\n",
    "         \"AdaBoost         \",\n",
    "         \"Neural Network   \"]\n",
    "\n",
    "classifiers = [\n",
    "        ExtraTreesClassifier(),\n",
    "        GradientBoostingClassifier(),\n",
    "        LogisticRegressionCV(),\n",
    "        LogisticRegression(),\n",
    "        SVC(),\n",
    "        SGDClassifier(),\n",
    "        NuSVC(),\n",
    "        DecisionTreeClassifier(),\n",
    "        ExtraTreeClassifier(),\n",
    "        CalibratedClassifierCV(),\n",
    "        KNeighborsClassifier(3),\n",
    "        BaggingClassifier(),\n",
    "        SVC(kernel=\"linear\", C=0.025),\n",
    "        SVC(gamma=2, C=1),\n",
    "        DecisionTreeClassifier(max_depth=5),\n",
    "        RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "        Perceptron(),\n",
    "        AdaBoostClassifier(),\n",
    "        MLPClassifier(alpha=1),\n",
    "        ]\n",
    "\n",
    "result = zip(names, classifiers)\n",
    "\n",
    "for n, cls in result:\n",
    "    pipeline = make_pipeline(\n",
    "        TfidfVectorizer(),\n",
    "        cls\n",
    "    )\n",
    "    results = cross_validate(pipeline, Xtrain, Ytrain)\n",
    "    scores = results['test_score']\n",
    "    avg(n, scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifiers: \n",
    "\n",
    "//  probably dense data\n",
    "BayesianGaussianMixture\n",
    "BernoulliNB\n",
    "ComplementNB\n",
    "GaussianMixture\n",
    "GaussianNB\n",
    "MultinomialNB\n",
    "QuadraticDiscriminantAnalysis\n",
    "\n",
    "// check with dense data, not working with sparse matrix \n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
