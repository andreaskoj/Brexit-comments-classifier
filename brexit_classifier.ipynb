{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LCqSmHmtbo6y"
   },
   "source": [
    "Tips: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OEl3S8BJbo65"
   },
   "source": [
    "https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "\n",
    "\n",
    "1 Word removal-experiment with word removals both in high frequency and low frequency space\n",
    "\n",
    "2 Taxonomy- Ordered heirarchial taxonomy can help you reduce entropy in content. Similar to what decision trees accomplish. This is SUPER unappreciated in most text analytics literature\n",
    "\n",
    "3 Lemmatization- it sometimes helps improve structure of text\n",
    "\n",
    "4 Sampling - Are your classes balanced? if not resample, create synthetic data to populate low frequency classes\n",
    "\n",
    "5 ensemble/boosting/bagging methods- Have you tried meta algos to improve your performance?\n",
    "\n",
    "6 Data Quality check- Good data is like good food. Have you checked if you have any useless data? pruning data set helps improve structure a lot!\n",
    "\n",
    "7 Hyper Parameter tuning- Did you exhaustively search the subspaces to fine tune the model. Most models show upto 10% improvement by just hyperparameter tuning.\n",
    "\n",
    "8 Word Gram conversion- you can improve feature set by converting important bigrams and trigrams into unigrams \"food processing\" can be changed to \"food_processing\"\n",
    "\n",
    "9 Data preparation- Count vectors, tf-idf, word2vec .. et al.. Naive bayes seems to work better Count vectors,\n",
    "\n",
    "10 Sample length- This is a quirk i noticed in text mining. if you are classifying sentences ranging from few words to lets say a paragraph. With more additional words algos like naive bayes dont work well due lack of convergence arising from positive hits they get from all words for more that one class to put it naively (pun intended :P)\n",
    "\n",
    "11 Last point I can think is instead of getting class assignment get class probabilities and use that itself a feature in your ensembles. It seems to work in a variety of cases.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Pro/against Brexit comments classfier - DAT340 / DIT866</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You should include instructions for running your classifier on a separate test set\n",
    "\n",
    "\n",
    "    How much consensus is there between annotators of the dataset?\n",
    "    -> waiting for 2nd round of annotation\n",
    "    How do you represent your data as features?\n",
    "    Did you process the features in any way?\n",
    "    Did you bring in any additional sources of data?\n",
    "    How did you select which learning algorithms to use?\n",
    "    Did you try to tune the hyperparameters of the learning algorithm, and in that case how?\n",
    "    How do you evaluate the quality of your system?\n",
    "    How well does your system compare to a stupid baseline?\n",
    "    Can you say anything about the errors that the system makes? For a classification task, you may consider a confusion matrix.\n",
    "    Is it possible to say something about which features the model considers important? (Whether this is possible depends on the type of classifier you are using.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.tree import ExtraTreeClassifier, DecisionTreeClassifier\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Read the data from the file </h3>\n",
    "Splits data into training and test set (80% - training set, 20% - testing set) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1KYinW-2bo7B"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements in training set: 5787\n",
      "Elements in test set: 1447\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(filename, encoding = 'utf-8') as f: \n",
    "        for l in f:\n",
    "            cols = l.split('\\t')\n",
    "            Y.append(cols[0]) #outputs\n",
    "            X.append(cols[1]) #features   \n",
    "    return X, Y\n",
    "\n",
    "trainDataSet = \"datasets/a2_train.tsv\" \n",
    "smallDataSet = \"datasets/a2_first_sample.tsv\"\n",
    "testSetFraction = 0.2\n",
    "\n",
    "X,Y = read_data(trainDataSet)\n",
    "#X,Y = read_data(smallDataSet)\n",
    "\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "\n",
    "print(\"Elements in training set: \" + str(len(Xtrain)))\n",
    "print(\"Elements in test set: \" + str(len(Xtest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization - preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "import re\n",
    "\n",
    "porter = PorterStemmer()\n",
    "lancester = LancasterStemmer()\n",
    "\n",
    "#[porter.stem(word) for word in text.split()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class TfidfVectorizerCustom(TfidfVectorizer):\n",
    "\n",
    "\n",
    "\n",
    "#print(TfidfVectorizerCustom.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "enFjwGHCbo7O"
   },
   "outputs": [],
   "source": [
    "# prints best classifiers         \n",
    "def bestClassifier(scores):\n",
    "    print(type(scores))\n",
    "   \n",
    "     \n",
    "    for s in scores:\n",
    "        print(s[0] + \"\\t\" + str((s[1][0])*100)[:6]+\" \"+str((s[1][1])*100)[:6] +\" \"+ str((s[1][2])*100)[:6])\n",
    "        print(len(scores))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nIELpEo7bo7X"
   },
   "source": [
    "### Find the best classfier (sparse matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YkmWkE1tbo7Z",
    "outputId": "f3f33861-c7fa-4679-f162-551e122f8845"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier:    \t76.36%\n",
      "RBF SVM:          \t76.08%\n",
      "LogisticRegression:\t76.01%\n",
      "LogisticRegressionCV:\t75.88%\n",
      "CalibratedClasCV:\t75.60%\n",
      "ExtraTreesClassifier:\t72.49%\n",
      "Perceptron:       \t72.42%\n",
      "NuSVC:            \t71.94%\n",
      "GradientBoostingClas:\t71.45%\n",
      "AdaBoost:        \t70.14%\n",
      "BaggingClassifier:\t69.52%\n",
      "Nearest Neighbors:\t65.79%\n",
      "DecisionTreeClass:\t64.82%\n",
      "Decision Tree:    \t64.68%\n",
      "Linear SVM:      \t63.23%\n",
      "ExtraTreeClassifier:\t63.09%\n",
      "SVC:              \t52.03%\n",
      "Random Forest:    \t51.96%\n"
     ]
    }
   ],
   "source": [
    "names = [\"ExtraTreesClassifier:\",\n",
    "         \"GradientBoostingClas:\",\n",
    "         \"LogisticRegressionCV:\",\n",
    "         \"LogisticRegression:\",\n",
    "         \"SVC:              \",\n",
    "         \"SGDClassifier:    \",\n",
    "         \"NuSVC:            \",\n",
    "         \"DecisionTreeClass:\",\n",
    "         \"ExtraTreeClassifier:\",\n",
    "         \"CalibratedClasCV:\",\n",
    "         \"Nearest Neighbors:\",\n",
    "         \"BaggingClassifier:\",\n",
    "         \"Linear SVM:      \",\n",
    "         \"RBF SVM:          \",\n",
    "         \"Decision Tree:    \",\n",
    "         \"Random Forest:    \",\n",
    "         \"Perceptron:       \",\n",
    "         \"AdaBoost:        \",\n",
    "         #\"Neural Network:   \"\n",
    "         ]\n",
    "\n",
    "classifiers = [\n",
    "        ExtraTreesClassifier(),\n",
    "        GradientBoostingClassifier(),\n",
    "        LogisticRegressionCV(),\n",
    "        LogisticRegression(),\n",
    "        SVC(),\n",
    "        SGDClassifier(),\n",
    "        NuSVC(),\n",
    "        DecisionTreeClassifier(),\n",
    "        ExtraTreeClassifier(),\n",
    "        CalibratedClassifierCV(),\n",
    "        KNeighborsClassifier(3),\n",
    "        BaggingClassifier(),\n",
    "        SVC(kernel=\"linear\", C=0.025),\n",
    "        SVC(gamma=2, C=1),\n",
    "        DecisionTreeClassifier(max_depth=5),\n",
    "        RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "        Perceptron(),\n",
    "        AdaBoostClassifier(),\n",
    "        #MLPClassifier(alpha=1),\n",
    "        ]\n",
    "\n",
    "result = zip(names, classifiers)\n",
    "scores = []\n",
    "\n",
    "for n , cls in result:\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('clf', cls)])\n",
    "    \n",
    "    results = cross_validate(pipeline, Xtrain, Ytrain)\n",
    "    \n",
    "    #evaluating\n",
    "    pipeline.fit(Xtrain, Ytrain)\n",
    "    Yguess = pipeline.predict(Xtest)\n",
    "    scores.append((n, (accuracy_score(Ytest, Yguess))))\n",
    "\n",
    "    #sorting results\n",
    "sortedResults = sorted(scores, key=lambda tup: tup[1], reverse=True)\n",
    "    \n",
    "for item in sortedResults:\n",
    "    print(item[0] + '\\t' + str((item[1]) * 100 )[:5]+ '%')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Brexit - Classifier (default tokenizer).ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
